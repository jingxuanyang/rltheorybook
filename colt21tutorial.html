<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-46766886-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-46766886-4');
  </script>


<script type="text/javascript">
  function visibility_on(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'none')
           e.style.display = 'block';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'none')
           e.style.display = 'block';
  }
  function visibility_off(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'block')
           e.style.display = 'none';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'block')
           e.style.display = 'none';
  }
  function toggle_visibility(id) {
      var e = document.getElementById(id+"_text");
      if(e.style.display == 'inline')
         e.style.display = 'block';
      else
         e.style.display = 'inline';
      var e = document.getElementById(id+"_img");
      if(e.style.display == 'inline')
         e.style.display = 'block';
      else
         e.style.display = 'inline';
  }
  function toggle_vis(id) {
      var e = document.getElementById(id);
      if (e.style.display == 'none')
          e.style.display = 'inline';
      else
          e.style.display = 'none';
  }
</script>


  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>COLT 2021 RL Tutorial</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>


<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>COLT 2021 Tutorial: Statistical Foundations of Reinforcement Learning</name>
              </p>
              
              <br>The past decade has seen tremendous interest in sequential decision making under 
              uncertainty, a broad class of problems involving an agent interacting with an unknown 
              environment to accomplish some goal. Reinforcement learning approaches to addressing 
              these problems have led to recent AI breakthroughs in game playing, robotics, and 
              elsewhere. Inspired by these empirical demonstrations, many researchers from the learning
              theory community have turned their attention to reinforcement learning, in an attempt to 
              better understand these problems and develop new algorithmic principles. Their efforts 
              have led to a more-modern statistical foundation for reinforcement learning that places 
              an emphasis on non-asymptotic characterizations via global convergence, sample 
              complexity, and regret analyses. </br>

              <br>This tutorial will provide an overview of this emerging theory with a focus on the 
              most challenging online exploration setting. The tutorial is organized into three sections:
          <ol>
            <li>Part 1 will cover the necessary background and definitions. We focus here on the most 
              basic setting of tabular Markov Decision Processes and consider problems of increasing 
              difficulty: from planning, to optimization with an exploratory distribution, to online
              exploration. We will present two algorithms: Natural Policy Gradient (NPG) for the optimization 
              problem and UCB-Value Iteration (UCB-VI) for exploration, along with their guarantees. </li>
            <li>Part 2 will be a recitation/practice session. We have prepared a problem set that covers 
              the analyses of NPG and UCB-VI in detail highlighting key lemmas that are broadly useful 
              in reinforcement learning, as well as technical connections to related fields. This session 
              will take place in gather.town, and a number of experts in the field will be available to 
              assist on the problem set or to answer other questions.</li>
           <li>Part 3 will focus on online exploration beyond the tabular setting, where function 
              approximation is required for generalization. Here we will provide a tour of the zoo 
              of RL models and complexity measures that enable tractable learning, as well as some 
              statistical barriers and algorithms. We will close with some open problems and future 
              directions. </li>
          </ol>


          <br>The tutorial will be accessible to all COLT attendees. No background knowledge in RL 
          is required, but we do expect tutorial attendees to be comfortable with the standard 
          mathematical tools used in learning theory research, such as concentration inequalities 
          and some linear algebra. </br>
            </td>
          </tr>
        </table>

        

    

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Staff</heading>
              </p>
              <p>
                <br><strong>Main Presenters</strong>: <a href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a> (MSR NYC), <a href="https://wensun.github.io">Wen Sun</a> (Cornell) </br>
                <br><strong>Discussion Leads</strong>: <a href="https://cdann.net">Chris Dann</a>, <a href="https://florencefeng.github.io">Fei Feng</a>, <a href="https://people.orie.cornell.edu/cleeyu/">Christina Yu</a>, <a href="https://azanette.com">Andrea Zanette</a> </br>
                <br><strong>Tutorial time</strong>: August 5th 7:15 - 10:15 am (Mountain Time)  </br>
                
                
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>How to attend</heading>
              </p>
              <p>
                 The tutorial will be entirely virtual, via Zoom and Gather.town.
                 Attendees must register for COLT 2021, which is free for students. You can register for the conference <a href="http://www.learningtheory.org/colt2021/">here</a>. 
                 Conference attendees can join the tutorial from the COLT Gather.town space
              </p>
            </td>
          </tr>
        </table>



              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                  <td width="100%" valign="middle">
                    <p align="center">
                      <heading>Schedule</heading>
                    </p>
                    
                    <table>
              <tbody>
                        <tr height="50" bgcolor="#F8F8FF">
                          <td width="25%"><strong>Time</strong></td>
                          <td></td>
                          <td><strong>Session</strong></td>
                          <td>Materials</td>
                          <!-- <td>Slides/Problem Sets </td> -->
                        </tr>
                
                        <tr height="50">
                          <td>7:15am-8am</td>
                          <td></td>
                          <td> <strong>Fundamentals</strong>: Markov Decision Processes, Natural policy gradient, and Upper confidence bound exploration</td>
                          <td><a href="./colt21_part1.pdf">Slides</a><br />
                            <a href="https://www.youtube.com/watch?v=hIuIdWH4qsc">Video</a></td>
                          <!-- <td></td> -->
                        </tr>
      
                        <tr height="50" bgcolor="#F8F8FF">
                            <td>8:15am-9am</td>
                            <td></td>
                            <td> <strong>Recitation / Practice: </strong>Analysis of NPG and UCB-VI (in Gather)</td>
                            <td><a href="./colt21_exercises.pdf">Exercises</a><br />
                                <a href="./colt21_solutions.pdf">Solutions</a></td>
                            <!-- <td></td> -->
                          </tr>
  
                        
                        <tr height="50">
                            <td>9:15am-10:15am</td>
                            <td></td>
                            <td> <strong>Generalization in RL</strong>: Linear methods, Bellman rank, Bilinear classes, Representation learning</td>
                            <td><a href="./colt21_part3.pdf">Slides</a><br />
                              <a href="https://www.youtube.com/watch?v=_GcotKckeMM">Video</a></td>
                            <!-- <td></td> -->
                        </tr> 
 
      
                
                     </tbody></td></tr>
                    </table>
      
      
      
                  </td>
                </tr>
              </table>
      
      
              
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
              <heading>Relevant Resources</heading>
              </p>
              <p><strong><a href="https://sites.google.com/view/rltheoryseminars/home">RL Theory Seminar series (virtual)</a></strong>: Keep up with advances in RL theory by joining the mailing list and tuning in.</p>
              <p><strong>Primary references</strong></p>
              <ul>
                <li>Alekh Agarwal, Nan Jiang, Sham M. Kakade, Wen Sun. Reinforcement Learning Theory and Algorithms. Monograph available <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">here</a>.</li>                
                <li>Alekh Agarwal, Sham M. Kakade, Gaurav Mahajan, Jason D. Lee. On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift. COLT 2020. </li>
                <li>Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. ICML 2020</li>                
                <li>Mohammad Gheshlaghi Azar, Ian Osband, Remi Munos. Minimax Regret Bounds for Reinforcement Learning. ICML 2017.</li>
                <li>Yuanhao Wang, Ruosong Wang, Sham M. Kakade. An Exponential Lower Bound for Linearly-Realizable MDPs with Constant Suboptimality Gap. arXiv 2021.</li>
                <li>Chi Jin, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan. Provably Efficient Reinforcement Learning with Linear Function Approximation. COLT 2020.</li>
                <li>Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire. Contextual Decision Processes with Low Bellman Rank are PAC-Learnable. ICML 2017.</li>
                <li>Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, Ruosong Wang. Bilinear Classes: A Structural Framework for Provable Generalization in RL. ICML 2021.</li>
                <li>Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, John Langford. Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning. ICML 2020.</li>
                <li>Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, Wen Sun. FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs. NeurIPS 2020.</li>
              </ul>
              <p><strong>Other references</strong></p> 
              <ul>
                <li>Martin L. Puterman. Markov Decision Processes: Discrete and Stochastic Dynamic Programming. John Wiley & Sons, 2014. </li>
                <li>Michael Kearns, Yishay Mansour, Andrew Ng. A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Machine Learning 2002.</li>
                
                <li><strong>MDPs with linear structures</strong>:</li>
                <ul>
                <li>Zheng Wen, Benjamin Van Roy. Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization. Mathematics of Operations Research, 2017.</li>
                <li>Simon S. Du, Yuping Luo, Ruosong Wang, Hanrui Zhang. Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle. NeurIPS 2019.</li>
                <li>Tor Lattimore, Csaba Szepesvari, Gellert Weisz. Learning with good feature representations in bandits and in rl with a generative model. ICML 2020.</li>
                <li>Ruosong Wang, Dean P. Foster, Sham M. Kakade. What are the Statistical Limits of Offline RL with Linear Function Approximation? ICLR 2021.</li>
                <li>Gellert Weisz, Philip Amortila, Csaba Szepesvári. Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions. ALT 2021.</li>
                <li>Gellert Weisz, Philip Amortila, Barnabas Janzer, Yasin Abbasi-Yadkori, Nan Jiang, Csaba Szepesvari. On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function. arXiv 2021.</li>
                <li>Lin Yang, Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. ICML 2020.</li>
                <li>Aditya Modi, Nan Jiang, Ambuj Tewari, Satinder Singh. Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles. AISTATS 2020.</li>
                <li>Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, Lin Yang. Model-based reinforcement learning with value-targeted regression. ICML 2020</li>
                <li>Dongruo Zhou, Quanquan Gu, Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. COLT 2021</li>
                <li>Gergely Neu, Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning. NeurIPS 2020</li>
                <li>Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, Emma Brunskill. Learning Near Optimal Policies with Low Inherent Bellman Error. ICML 2020.</li>
                </ul>
                
                <li><strong>Function approximation with Eluder dimension</strong>:</li>
                <ul>
                <li>Daniel Russo, Benjamin Van Roy. Eluder Dimension and the Sample Complexity of Optimistic Exploration. NeurIPS 2013.</li>
                <li>Gene Li, Pritish Kamath, Dylan J. Foster, Nathan Srebro. Eluder Dimension and Generalized Rank. arXiv 2021.</li>
                </ul>
                
                <li><strong>Nonlinear function approximation</strong>:</li>
                <ul>
                <li>Antos, Andras, Csaba Szepesvari, Remi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning 2008</li>
                <li>Kefan Dong, Jiaqi Yang, Tengyu Ma. Provable Model-based Nonlinear Bandit and Reinforcement Learning: Shelve Optimism, Embrace Virtual Curvature. arXiv 2021.</li>
                <li>Chi Jin, Qinghua Liu, Sobhan Miryoosefi. Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. </li>
                <li>Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford. Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. COLT 2019</li>
                <li>Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, John Langford. Provably efficient RL with Rich Observations via Latent State Decoding. ICML 2019.</li>
                <li>Fei Feng, Ruosong Wang, Wotao Yin, Simon S. Du, Lin F. Yang. Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning. NeurIPS 2020.</li>
                <li>Dylan J. Foster, Alexander Rakhlin, David Simchi-Levi, Yunzong Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. COLT 2021</li>                
                <li>Tianhao Wu, Yunchang Yang*, Simon S. Du, Liwei Wang. On Reinforcement Learning with Adversarial Corruption and Its Application to Block MDP. ICML 2021.</li>
                <li>Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal. Model-free Representation Learning and Exploration in Low-rank MDPs. arXiv 2021.</li>
                <li>Sarah Dean, Benjamin Recht. Certainty Equivalent Perception-Based Control. L4DC 2021.</li>
                <li>Zakaria Mhammedi, Dylan J. Foster, Max Simchowitz, Dipendra Misra, Wen Sun, Akshay Krishnamurthy, Alexander Rakhlin, John Langford. Learning the Linear Quadratic Regulator from Nonlinear Observations. NeurIPS 2020.</li>
                <li>Dipendra Misra, Qinghua Liu, Chi Jin, John Langford. Provable Rich Observation Reinforcement Learning with Combinatorial Latent States. ICLR 2021.</li>
                </ul>
                
                <li><strong>Policy gradient methods</strong>:</li>
                <ul>
                <li>Jalaj Bhandari, Daniel Russo. Global optimality guarantees for policy gradient methods</li>
                <li>Matthieu Geist, Bruno Scherrer, Olivier Pietquin. A theory of regularized markov decision processes. ICML 2019</li>
                <li>Alekh Agarwal, Mikael Henaff, Sham Kakade, Wen Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. NeurIPS 2020</li>
                <li>Andrea Zanette, Ching-An Cheng, Alekh Agarwal. Cautiously optimistic policy optimization and exploration with linear function approximation. COLT 2021</li>
                </ul>
                
              </ul>
                  </td>
                </tr>
              </table>     

        


     


        
        </td>
    </tr>
  </table>
</body>

</html>
