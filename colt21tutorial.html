<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-46766886-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-46766886-4');
  </script>


<script type="text/javascript">
  function visibility_on(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'none')
           e.style.display = 'block';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'none')
           e.style.display = 'block';
  }
  function visibility_off(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'block')
           e.style.display = 'none';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'block')
           e.style.display = 'none';
  }
  function toggle_visibility(id) {
      var e = document.getElementById(id+"_text");
      if(e.style.display == 'inline')
         e.style.display = 'block';
      else
         e.style.display = 'inline';
      var e = document.getElementById(id+"_img");
      if(e.style.display == 'inline')
         e.style.display = 'block';
      else
         e.style.display = 'inline';
  }
  function toggle_vis(id) {
      var e = document.getElementById(id);
      if (e.style.display == 'none')
          e.style.display = 'inline';
      else
          e.style.display = 'none';
  }
</script>


  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>COLT 2021 RL Tutorial</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>


<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>COLT 2021 Tutorial: Statistical Foundations of Reinforcement Learning</name>
              </p>
              
              <br>The past decade has seen tremendous interest in sequential decision making under 
              uncertainty, a broad class of problems involving an agent interacting with an unknown 
              environment to accomplish some goal. Reinforcement learning approaches to addressing 
              these problems have led to recent AI breakthroughs in game playing, robotics, and 
              elsewhere. Inspired by these empirical demonstrations, many researchers from the learning
              theory community have turned their attention to reinforcement learning, in an attempt to 
              better understand these problems and develop new algorithmic principles. Their efforts 
              have led to a more-modern statistical foundation for reinforcement learning that places 
              an emphasis on non-asymptotic characterizations via global convergence, sample 
              complexity, and regret analyses. </br>

              <br>This tutorial will provide an overview of this emerging theory with a focus on the 
              most challenging online exploration setting. The tutorial is organized into three sections:
          <ol>
            <li>Part 1 will cover the necessary background and definitions. We focus here on the most 
              basic setting of tabular Markov Decision Processes and consider problems of increasing 
              difficulty: from planning, to optimization with an exploratory distribution, to online
              exploration. We will present two algorithms: Natural Policy Gradient for the optimization 
              problem and UCB-Value Iteration for exploration, along with their guarantees. </li>
            <li>Part 2 will be organized in smaller discussion groups led by the tutorial presenters 
              and other researchers in the field. We will cover the analyses for NPG and UCB-VI in 
              detail, highlighting key lemmas that are broadly useful in reinforcement learning, as 
              well as technical connections to related fields. </li>
            <li>Part 3 will focus on online exploration beyond the tabular setting, where function 
              approximation is required for generalization. Here we will provide a tour of the zoo 
              of RL models and complexity measures that enable tractable learning, as well as some 
              statistical barriers. We will close with some open problems and future directions. </li>
          </ol>


          <br>The tutorial will be accessible to all COLT attendees. No background knowledge in RL 
          is required, but we do expect tutorial attendees to be comfortable with the standard 
          mathematical tools used in learning theory research, such as concentration inequalities 
          and some linear algebra. </br>
            </td>
          </tr>
        </table>

        

    

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Staff</heading>
              </p>
              <p>
                <br><strong>Main Presenters</strong>: <a href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a> (MSR NYC), <a href="https://wensun.github.io">Wen Sun</a> (Cornell) </br>
                <br><strong>Discussion Leads</strong>: <a href="https://cdann.net">Chris Dann</a>, <a href="https://florencefeng.github.io">Fei Feng</a>, <a href="https://people.orie.cornell.edu/cleeyu/">Christina Yu</a>, <a href="https://azanette.com">Andrea Zanette</a> </br>
                <br><strong>Tutorial time</strong>: August 5th 7:15 - 10:15 am (Mountain Time)  </br>
                
                
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <heading>Zoom / Gather Town Information</heading>
              </p>
              <p>
                 TBD
              </p>
            </td>
          </tr>
        </table>



              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                  <td width="100%" valign="middle">
                    <p align="center">
                      <heading>Schedule (tentative)</heading>
                    </p>
                    
                    <table>
              <tbody>
                        <tr height="50" bgcolor="#F8F8FF">
                          <td></td>
                          <td></td>
                          <td> <strong>Session</strong></td>
                          <!-- <td>Reading</td> -->
                          <!-- <td>Slides/Problem Sets </td> -->
                        </tr>
                
                        <tr height="50">
                          <td>7:15am-8am</td>
                          <td></td>
                          <td> <strong>Fundamentals</strong>: Markov Decision Processes, Natural policy gradient, and Upper confidence bound exploration</td>
                          <!-- <td></td> -->
                          <!-- <td></td> -->
                        </tr>
      
                        <tr height="50" bgcolor="#F8F8FF">
                            <td>8:15am-9am</td>
                            <td></td>
                            <td> <strong>Recitation / Practice: </strong>Analysis of NPG and UCB-VI<br /> <a href="./exercises.pdf">Exercises</a> and <a href="./solutions.pdf">Solutions</a></td>
                            <!-- <td></td> -->
                            <!-- <td></td> -->
                          </tr>
  
                        
                        <tr height="50">
                            <td>9:15am-10:15am</td>
                            <td></td>
                            <td> <strong>Generalization in RL</strong>: Linear methods, Bellman rank, Bilinear classes, Representation learning</td>
                            <!-- <td></td> -->
                            <!-- <td></td> -->
                        </tr> 
 
      
                
                     </tbody></td></tr>
                    </table>
      
      
      
                  </td>
                </tr>
              </table>
      
      
              
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
              <heading>Relevant Notes / Books / Papers</heading>
              </p>
              <p>Books and Monographs</p> 
              <ul>
                <li>Martin L. Puterman. Markov Decision Processes: Discrete and Stochastic Dynamic Programming. John Wiley & Sons, 2014. </li>
                <li>Alekh Agarwal, Nan Jiang, Sham M. Kakade, Wen Sun. Reinforcement Learning Theory and Algorithms. Monograph available <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">here</a>.</li>
              </ul>
              <p>References for part 1</p>
              <ul>
                <li>Alekh Agarwal, Sham M. Kakade, Gaurav Mahajan, Jason D. Lee. On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift. COLT 2020. </li>
                <li>Mohammad Gheshlaghi Azar, Ian Osband, Remi Munos. Minimax Regret Bounds for Reinforcement Learning. ICML 2017.</li>
              </ul>
              <p>References for part 3</p>
              <ul>
                <li>Michael Kearns, Yishay Mansour, Andrew Ng. A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Machine Learning 2002.</li>
                <li>Zheng Wen, Benjamin Van Roy. Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization. Mathematics of Operations Research, 2017.</li>
                <li>Simon S. Du, Yuping Luo, Ruosong Wang, Hanrui Zhang. Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle. NeurIPS 2019.</li>
                <li>Tor Lattimore, Csaba Szepesvari, Gellert Weisz. Learning with good feature representations in bandits and in rl with a generative model. ICML 2020.</li>
                <li>Ruosong Wang, Dean P. Foster, Sham M. Kakade. What are the Statistical Limits of Offline RL with Linear Function Approximation? ICLR 2021.</li>
                <li>Gellert Weisz, Philip Amortila, Csaba Szepesvári. Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions. ALT 2021.</li>
                <li>Yuanhao Wang, Ruosong Wang, Sham M. Kakade. An Exponential Lower Bound for Linearly-Realizable MDPs with Constant Suboptimality Gap. arXiv 2021.</li>
                <li>Gellért Weisz, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, Csaba Szepesvári. On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function. arXiv 2021.</li>
                <li>Chi Jin, Zhuoran Yang, Zhaoran Wang, Michael I. Jordan. Provably Efficient Reinforcement Learning with Linear Function Approximation. COLT 2020.</li>
                <li>Daniel Russo, Benjamin Van Roy. Eluder Dimension and the Sample Complexity of Optimistic Exploration. NeurIPS 2013.</li>
                <li>Kefan Dong, Jiaqi Yang, Tengyu Ma. Provable Model-based Nonlinear Bandit and Reinforcement Learning: Shelve Optimism, Embrace Virtual Curvature. arXiv 2021.</li>
                <li>Gene Li, Pritish Kamath, Dylan J. Foster, Nathan Srebro. luder Dimension and Generalized Rank. arXiv 2021.</li>
                <li>Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, Emma Brunskill. Learning Near Optimal Policies with Low Inherent Bellman Error. ICML 2020.</li>
                <li>Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire. Contextual Decision Processes with Low Bellman Rank are PAC-Learnable. ICML 2017.</li>
                <li>Chi Jin, Qinghua Liu, Sobhan Miryoosefi. Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. ICML 2021.</li>
                <li>Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, Ruosong Wang. Bilinear Classes: A Structural Framework for Provable Generalization in RL. ICML 2021.</li>
                <li>Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, John Langford. Provably efficient RL with Rich Observations via Latent State Decoding. ICML 2019.</li>
                <li>Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, John Langford. Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning. ICML 2020.</li>
                <li>Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, Wen Sun. FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs. NeurIPS 2020.</li>
              </ul>
                  </td>
                </tr>
              </table>     

        


     


        
        </td>
    </tr>
  </table>
</body>

</html>
